diff --git a/ppo.py b/ppo.py
index 10fa5ee..f9cf513 100644
--- a/ppo.py
+++ b/ppo.py
@@ -1,19 +1,27 @@
 # ppo - no noise
 # /opt/anaconda3/lib/python3.12
+
+# setting the path for python3.12
 import sys
 sys.path.insert(0, '/opt/anaconda3/lib/python3.12/site-packages')
+
+# importing packages
 import gymnasium as gym
 import numpy as np
-#import wandb
+import wandb
 import logging
+
+# importing tools for evalutaion
 from stable_baselines3.common.callbacks import EvalCallback
 from stable_baselines3.common.evaluation import evaluate_policy
 # from wand.integration.sb3 import WandbCallback
+
+# importing PPO and logger tools
 from stable_baselines3.common.logger import configure
-# import pdb
 from stable_baselines3 import PPO
 from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
 
+# importing environment packages
 import ale_py
 gym.register_envs(ale_py)
 
@@ -21,6 +29,7 @@ gym.register_envs(ale_py)
 seed = 42
 np.random.seed(seed)
 
+# creating the environment for training
 def make_env():
     env = gym.make("ALE/Breakout-v5", render_mode="rgb_array")
     env = gym.wrappers.RecordEpisodeStatistics(env)  # record stats such as returns
@@ -28,19 +37,29 @@ def make_env():
     env.action_space.seed(seed)
     return env
 
-# run = wandb.init(
-#     project="atari_ppo_breakout",
-#     config = {"policy": "PPO_MlpPolicy", "learning_rate": 0.000827, "gamma":0.96979, "gae_lambda":0.92680, "ent_coef":.0000199},
-#     monitor_gym=True,
-#     sync_tensorboard=True,
-#     save_code=True
+# creating the environment for testing
+def make_env_pong():
+    env = gym.make("ALE/Pong-v5", render_mode="rgb_array")
+    env = gym.wrappers.RecordEpisodeStatistics(env)  # record stats such as returns
+    env.reset(seed=seed) 
+    env.action_space.seed(seed)
+    return env
+
+# saving important configs to wandb (data logger)
+run = wandb.init(
+    project="atari_ppo_breakout",
+    config = {"policy": "PPO_MlpPolicy", "learning_rate": 0.000827, "gamma":0.96979, "gae_lambda":0.92680, "ent_coef":.0000199},
+    monitor_gym=True,
+    sync_tensorboard=True,
+    save_code=True
 
-# )
+)
 
+# establishing environment pt.2
 env = DummyVecEnv([make_env])
-env = VecVideoRecorder(env, f"videos/test1", record_video_trigger = lambda x: x % 2000 == 0, video_length=200) # f"videos/{run.id}"
+env = VecVideoRecorder(env, f"videos/{run.id}", record_video_trigger = lambda x: x % 2000 == 0, video_length=200) 
 
-# Instantiate the agent without noise
+# Instantiate the agent without noise (things in comments are logger debuggers in case it is necessary)
 #tmp_path = "/tmp/sb3_log/"
 #new_logger = configure(tmp_path, ["stdout", "csv", "tensorboard"])
 model = PPO("MlpPolicy", env, learning_rate= 0.000827, gamma = 0.96979, gae_lambda= 0.92680, ent_coef= 0.0000199, verbose=1, tensorboard_log=f"runs/ppo_breakout")
@@ -49,7 +68,7 @@ model = PPO("MlpPolicy", env, learning_rate= 0.000827, gamma = 0.96979, gae_lamb
 # Train the agent
 model.learn(total_timesteps= 300000,
             #callback=WandbCallback(gradient_save_freq=200, model_save_path=f"models/{run.id}", verbose=2)
-            callback=EvalCallback(env, best_model_save_path=f"models/best/test1", log_path="./evalLogs/", eval_freq=1000)
+            callback=EvalCallback(env, best_model_save_path=f"models/best/{run.id}", log_path="./evalLogs/", eval_freq=1000)
 )
 
 # Save the agent
@@ -61,23 +80,50 @@ del model # remove to demonstrate saving and loading
 # Load the agent
 model = PPO.load("atari_ppo_breakout")
 
-# Evaluate the agent
+# Evaluate the agent performance from training with the same game
 mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
-#wandb.log({"mean_reward": mean_reward, "std_reward": std_reward})
+wandb.log({"mean_reward": mean_reward, "std_reward": std_reward})
 
 print(f"mean reward: {mean_reward} +/- {std_reward}")
 obs = vec_env.reset()
 
+# continue testing agent performance of same game
 reward_total = 0
 for _ in range(1000): 
     action, _states = model.predict(obs)
     obs, rewards, dones, info = vec_env.step(action)
     reward_total += rewards
-    #wandb.log({"returns" : reward_total, "reward": rewards})
+    wandb.log({"returns" : reward_total, "reward": rewards})
     # print(rewards)
     if dones:
         reward_total = 0
         obs = vec_env.step.reset()
 
 print(reward_total)
-env.close()
\ No newline at end of file
+env.close()
+
+# establish new environment for Pong
+new_env = DummyVecEnv([make_env_pong])
+new_env = VecVideoRecorder(new_env, f"videos/{run.id}", record_video_trigger = lambda x: x % 2000 == 0, video_length=200) 
+
+# Evaluate the agent performance from Pong
+mean_reward, std_reward = evaluate_policy(model, new_env, n_eval_episodes=10)
+wandb.log({"mean_reward": mean_reward, "std_reward": std_reward})
+
+print(f"mean reward: {mean_reward} +/- {std_reward}")
+obs = new_env.reset()
+
+# continue testing agent performance of different game
+reward_total = 0
+for _ in range(1000): 
+    action, _states = model.predict(obs)
+    obs, rewards, dones, info = new_env.step(action)
+    reward_total += rewards
+    wandb.log({"returns" : reward_total, "reward": rewards})
+    # print(rewards)
+    if dones:
+        reward_total = 0
+        obs = new_env.step.reset()
+
+print(reward_total)
+new_env.close()
diff --git a/videos/test1/rl-video-step-0-to-step-200.mp4 b/videos/test1/rl-video-step-0-to-step-200.mp4
index 27112bd..596ec76 100644
Binary files a/videos/test1/rl-video-step-0-to-step-200.mp4 and b/videos/test1/rl-video-step-0-to-step-200.mp4 differ
