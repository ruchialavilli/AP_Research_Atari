diff --git a/ppo.py b/ppo.py
index 10fa5ee..5d5fcbb 100644
--- a/ppo.py
+++ b/ppo.py
@@ -4,7 +4,7 @@ import sys
 sys.path.insert(0, '/opt/anaconda3/lib/python3.12/site-packages')
 import gymnasium as gym
 import numpy as np
-#import wandb
+import wandb
 import logging
 from stable_baselines3.common.callbacks import EvalCallback
 from stable_baselines3.common.evaluation import evaluate_policy
@@ -28,14 +28,14 @@ def make_env():
     env.action_space.seed(seed)
     return env
 
-# run = wandb.init(
-#     project="atari_ppo_breakout",
-#     config = {"policy": "PPO_MlpPolicy", "learning_rate": 0.000827, "gamma":0.96979, "gae_lambda":0.92680, "ent_coef":.0000199},
-#     monitor_gym=True,
-#     sync_tensorboard=True,
-#     save_code=True
+run = wandb.init(
+    project="atari_ppo_breakout",
+    config = {"policy": "PPO_MlpPolicy", "learning_rate": 0.000827, "gamma":0.96979, "gae_lambda":0.92680, "ent_coef":.0000199},
+    monitor_gym=True,
+    sync_tensorboard=True,
+    save_code=True
 
-# )
+)
 
 env = DummyVecEnv([make_env])
 env = VecVideoRecorder(env, f"videos/test1", record_video_trigger = lambda x: x % 2000 == 0, video_length=200) # f"videos/{run.id}"
@@ -63,7 +63,7 @@ model = PPO.load("atari_ppo_breakout")
 
 # Evaluate the agent
 mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
-#wandb.log({"mean_reward": mean_reward, "std_reward": std_reward})
+wandb.log({"mean_reward": mean_reward, "std_reward": std_reward})
 
 print(f"mean reward: {mean_reward} +/- {std_reward}")
 obs = vec_env.reset()
@@ -73,7 +73,7 @@ for _ in range(1000):
     action, _states = model.predict(obs)
     obs, rewards, dones, info = vec_env.step(action)
     reward_total += rewards
-    #wandb.log({"returns" : reward_total, "reward": rewards})
+    wandb.log({"returns" : reward_total, "reward": rewards})
     # print(rewards)
     if dones:
         reward_total = 0
diff --git a/videos/test1/rl-video-step-0-to-step-200.mp4 b/videos/test1/rl-video-step-0-to-step-200.mp4
index 27112bd..a4ff0a7 100644
Binary files a/videos/test1/rl-video-step-0-to-step-200.mp4 and b/videos/test1/rl-video-step-0-to-step-200.mp4 differ
diff --git a/videos/test1/rl-video-step-2000-to-step-2200.mp4 b/videos/test1/rl-video-step-2000-to-step-2200.mp4
index d2b5379..318d836 100644
Binary files a/videos/test1/rl-video-step-2000-to-step-2200.mp4 and b/videos/test1/rl-video-step-2000-to-step-2200.mp4 differ
diff --git a/videos/test1/rl-video-step-4000-to-step-4200.mp4 b/videos/test1/rl-video-step-4000-to-step-4200.mp4
index d2b5379..318d836 100644
Binary files a/videos/test1/rl-video-step-4000-to-step-4200.mp4 and b/videos/test1/rl-video-step-4000-to-step-4200.mp4 differ
