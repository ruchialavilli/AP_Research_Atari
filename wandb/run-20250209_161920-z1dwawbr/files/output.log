Using cpu device
Wrapping the env in a VecTransposeImage.
Logging to runs/ppo_breakout/PPO_3
/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x1656e7740> != <stable_baselines3.common.vec_env.vec_video_recorder.VecVideoRecorder object at 0x1592cc620>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
Saving video to /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-0-to-step-200.mp4
Moviepy - Building video /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-0-to-step-200.mp4.
Moviepy - Writing video /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-0-to-step-200.mp4
/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.

Moviepy - Done !
Moviepy - video ready /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-0-to-step-200.mp4
  warnings.warn(
Saving video to /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-2000-to-step-2200.mp4
Moviepy - Building video /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-2000-to-step-2200.mp4.
Moviepy - Writing video /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-2000-to-step-2200.mp4
Traceback (most recent call last):                                                                                                                       

Moviepy - Done !
Moviepy - video ready /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-2000-to-step-2200.mp4
Saving video to /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-4000-to-step-4200.mp4
Moviepy - Building video /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-4000-to-step-4200.mp4.
Moviepy - Writing video /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-4000-to-step-4200.mp4

Moviepy - Done !
Moviepy - video ready /Users/ruchi/dev/AP_Research_Atari/videos/test1/rl-video-step-4000-to-step-4200.mp4
  File "/Users/ruchi/dev/AP_Research_Atari/ppo.py", line 50, in <module>
    model.learn(total_timesteps= 300000,
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
           ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 224, in collect_rollouts
    if not callback.on_step():
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step
    return self._on_step()
           ^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 464, in _on_step
    episode_rewards, episode_lengths = evaluate_policy(
                                       ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy
    actions, states = model.predict(
                      ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/base_class.py", line 556, in predict
    return self.policy.predict(observation, state, episode_start, deterministic)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/policies.py", line 368, in predict
    actions = self._predict(obs_tensor, deterministic=deterministic)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/policies.py", line 717, in _predict
    return self.get_distribution(observation).get_actions(deterministic=deterministic)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/policies.py", line 751, in get_distribution
    latent_pi = self.mlp_extractor.forward_actor(features)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/torch_layers.py", line 260, in forward_actor
    return self.policy_net(features)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1549, in _wrapped_call_impl
    def _wrapped_call_impl(self, *args, **kwargs):

KeyboardInterrupt
